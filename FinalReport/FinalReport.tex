\documentclass{article}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[margin=3cm]{geometry} % margins might need to change in the future check with professor adam 
\setlength{\parindent}{0pt}
\usepackage{pgfgantt}

\usepackage{times}
\usepackage{fancyhdr,graphicx,amsmath,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\include{pythonlisting}


\begin{document}

\begin{center}
    \Large \textcolor{red}{\textbf{Electronics and Computer Science} \\[0.1cm]} 

    \large \textcolor{red}{Faculty of Engineering and Physical Sciences \\[0.1cm]} 

    \textcolor{red}{University of Southampton \\[1cm]} 

    \vspace{1cm} 

    \textcolor{red}{\textbf{\large Ashwinkrishna Azhagesh} \\[0.5cm]}

    \textbf{25/03/2025} \\[1cm] 

    \textbf{\large An AI Approach to Chaotic Physical Systems: } \\[1cm]

    \vspace{0.5cm}

    Project supervisor: \textbf{Adam Peugeot} \\[0.3cm] 
    Second examiner: \textbf{David Millard} \\[1cm]

    Progress report submitted for the award of \\[0.1cm]

    \textbf{\large Bachelors of Science} 
\end{center}

\newpage

{\Huge \textbf{Abstract}}\\[1cm]

Empirical laws are mathematical generalisations found through observing the physical world. It has taken us centuries of gathering data, keen research along with repeated experiments, and no doubt plenty of talented scientists to discover these laws. Leading us to understand everything from the mysteries that govern the collision of two objects to the shape of the path planets thread upon.\\

Recent advances in neural networks including increases in computational power permit us to train models, that replicate, fasten and automate our discovery of empirical laws. This extends to even noisy chaotic systems such as the double pendulum. Combined with white box models, symbolic regression and explanable A.I., we can peer into the "mind," of how such models, process data and conclude their observations. Human congition is inherently finite in its capacity for thought and observational ability, has been historically overcome through the development of new tools such as the microscope. Similarly, congitive biases can be mitigated, by utillising artificial intelligence, which is a rapidly emerging technology capable of expanding our perception and analysis.\\       


\newpage

\fbox{\underline{\textbf{Statement of Originality}}}
\\[0.5cm]

- I have read and understood the ECS Academic Integrity information and the University’s
Academic Integrity Guidance for Students.\\

- I am aware that failure to act in accordance with the Regulations Governing Academic Integrity
may lead to the imposition of penalties which, for the most serious cases, may include
termination of programme.\\

- I consent to the University copying and distributing any or all of my work in any form and
using third parties (who may be based outside the EU/EEA) to verify whether my work
contains plagiarised material, and for quality assurance purposes.\\

\fbox{
\underline{\textbf{You must change the statements in the boxes if you do not agree with them.}\\
}}
\\[0.5cm]

We expect you to acknowledge all sources of information (e.g. ideas, algorithms, data) using
citations. You must also put quotation marks around any sections of text that you have copied
without paraphrasing. If any figures or tables have been taken or modified from another source,
you must explain this in the caption and cite the original source.\\

\fbox{
\underline{\textbf{I have acknowledged all sources, and identified any content taken from elsewhere.}\\
}}
\\[0.5cm]

If you have used any code (e.g. open-source code), reference designs, or similar resources that
have been produced by anyone else, you must list them in the box below. In the report, you must
explain what was used and how it relates to the work you have done.\\


\fbox{
\underline{\textbf{I have not used any resources produced by anyone else.}\\
}}
\\[0.5cm]

You can consult with module teaching staff/demonstrators, but you should not show anyone else
your work (this includes uploading your work to publicly-accessible repositories e.g. Github, unless
expressly permitted by the module leader), or help them to do theirs. For individual assignments,
we expect you to work on your own. For group assignments, we expect that you work only with
your allocated group. You must get permission in writing from the module teaching staff before
you seek outside assistance, e.g. a proofreading service, and declare it here.\\

\fbox{
\underline{\textbf{I did all the work myself, or with my allocated group, and have not helped anyone else.}\\
} }
\\[0.5cm]

We expect that you have not fabricated, modified or distorted any data, evidence, references,
experimental results, or other material used or presented in the report. You must clearly describe
your experiments and how the results were obtained, and include all data, source code and/or
designs (either in the report, or submitted as a separate file) so that your results could be
reproduced.\\

\fbox{
\underline{\textbf{The material in the report is genuine, and I have included all my data/code/designs.}\\
}}
\\[0.5cm]

We expect that you have not previously submitted any part of this work for another assessment.
You must get permission in writing from the module teaching staff before re-using any of your
previously submitted work for this assessment.\\

\fbox{
\underline{\textbf{I have not submitted any part of this work for another assessment.}\\
}}
\\[0.5cm]

If your work involved research/studies (including surveys) on human participants, their cells or
data, or on animals, you must have been granted ethical approval before the work was carried
out, and any experiments must have followed these requirements. You must give details of this in
the report, and list the ethical approval reference number(s) in the box below.\\

\fbox{
\underline{\textbf{My work did not involve human participants, their cells or data, or animals.}\\
}}
\\[0.5cm]


ECS Statement of Originality Template, updated August 2018, Alex Weddell aiofficer@ecs.soton.ac.uk\\


\newpage 

{\Huge \textbf{Abstract}}\\[1cm]


I would like to thank my supervisors, Professor Adam Peugeot and Professor David Millard, for all the help and advise I received throughout this project. \\


\newpage

\tableofcontents 

\newpage
\addcontentsline{toc}{section}{Abstract}
\addcontentsline{toc}{section}{Statement of Originality}



\section{Introduction: }

\subsection{Motivation: }

As there is more data being generated than ever before and new experiments, we need a systematic and au-
tomatic way to deduce various mathematical patterns and laws in these data. Through the use of symbolic
regression we can utilise these data, and in an explainable manner deduce various new physical laws. In this
research I have also extended this beyond physics and have applied this to biological data sets which is a novel
application of this method. Perhaps extend this beyond or add a sectionsaying this can also be applied to nlp
and that it can learn the rules in language and writing etc.
Talk a little about the way this is used outside of this niche use case, and in research, so of course I need to look
and research into this.\\

\section{Previos Work: }

\subsection{Literature Review: }

\subsection{ Introduction: }
  
Humanity has spent millennia observing the world, creating concepts that describe the variables, such as mass and force, to derive laws. In physics, like with
all human endeavours, new discoveries and ways of thought are based upon previous works, creating
a natural bias in the way new problems are approached. All existing theories, are therefore
somewhat biased, this combined with our pre-existing bias in our biological brains, can introduce some
hurdles to our future progress \cite{Wood2022} \cite{Schmidt2009}.\\

In the 17th Century, Kepler had gotten his hands on the word’s most precise data tables on the orbits
on planets, using this he spent close to half a decade, and after numerous unsuccessful
attempts, he had began a scientific revolution at the time, describing Mar’s orbit to be an ellipse \cite{kepler}.
In essence, scientists throughout history, much like Kepler, have spent a great deal of time, discovering
the right expressions to match the relevant data they have, this at it’s core is symbolic regression. Now,
a few centuries later, even with exponential increases in orders of magnitude in our capability to perform
calculations through computers, the process of discovering natural laws and the way to express them,
has to some extent resisted automation.\\


One of the core challenges of physics and artificial intelligence, is finding analytical relations automatically, discovering a symbolic expression that accurately matches the data from an unknown function.
This problem, due to it’s nature, is NP-hard \cite{Hope2023} in principle. The vastness of the space
of mathematical constants, adds to the difficulty. 
This literatire review aims to present the recent advances in discovery of emphirical laws through data powered by artificial intelligence. It focuses on methodoogies that dimish human bias through seeking solutions without assumptions. We will explore various techniques employed to achieve these goals, which includes reducing the search space, and analyse the effectivness of these methods.\\  

\begin{figure}[h] 
    \centering
    \includegraphics[width=7cm]{Sun_Mars_Orbit} 
    \caption{This is the orbit of Earth and Mars around the Sun.}
    \label{fig:Orbit} 
\end{figure}


\subsection{Symbolic Regression: }

Symbolic regression, is a technique that analyses and searches over the space of traceable mathematical
expressions to find the best fit for a data set. By not requiring prior information about the model,
it is unbiased. There are a plethora of various strategies that have been implemented in solving for
empirical laws \cite{Schmidt22009}, we will explore some of them below. It is also worth mentioning, that unlike other
well-known techniques for regression, (eg: neural networks), that are essentially black boxes, symbolic
regression, aims to extract white-box models and is easy to analyse.\\

\begin{center} 
  \textbf {\Large  Brute Force:}
\end{center}
Symbolic Regression (SR), is interpretable \cite{Aldeia2022}, unlike Neural Networks (NN), which are often considered more explainable. The difference is interpretability allows us to comprehend how the model works,
like observing how gears move in a glass box, while explainable means you get an overview of why a
certain output was achieved, even without knowing the full nuances of it’s inner workings.\\

There however, are some challenges associated with SR, in comparison to function fitting (NN). SR,
starts with nothing, a blank slate, and it has to learn the entire expression \cite{Cranmer2020}, unlike function fitting
which just tweaks an already existing function. The exponential search space \cite{Worm2014} , causes it to be
extremely computationally expensive to explore all possibilities. This combined with the face that,
most optimisation algorithms expect a smooth search space \cite{Makke2024}, however SR lack’s smooth interpolation, small changes in the potential solutions (expression), ie: $ x3 and x3 + 0.1$ can significantly alter
the the output. Finally, if the nature of the problem is badly posed \cite{Rivero2022}, there might potentially be
multiple solutions to the same data. Imagine trying to find a single polynomial equation with only two
points of data, the need to balance finding accurate expressions with finding the most simplistic and
generalisable fit, is sometimes troublesome.\\

The brute force approach of simply trying all possible combinations of symbolic expressions within some defined space. The model will subsequently increase the complexity over time, and will stop when either the fitting errors lowers below some defined limit or exceeds the upper limit of runtime. While in theory can solve all of our problems, in practise takes longer than the age of our universe to finish. In essence it's like searching for a singular drop in the ocean. Thankfully, there are some ways of pruning the search space, and drastically reducing the time taken to solve for the most accurate expression. \\ 

\begin{center} 
  \textbf {\Large Partial Derivatives:}
\end{center}

Partial derivatives, of some function f, with multiple variables such as x and y, is it's dervative with respect to one of those two variables, while the other variables in the function are kept constant. Formally, given a function with two or more variables, $f(x_1, x_2, \ldots, x_n) $, the partial derivative of $f$ with respect to $x_i$, where $x_i$ is some value $x$ in $(x_1, x_2, \ldots, x_i, \ldots, x_n)$, gives the rate of change of $f$ with respect to $x_i$. It is calculated by taking the ith derivative of $f$ with respect to $x_i$, whilst holding the other variables fixed. \cite{Stewart2012} \cite{Smith2012} \\

The partial derivative of a function $f(x,y)$ with respect to $x$ is denoted $\frac{\partial f}{\partial x}$ \cite{Kelly2021} and is defined: \\ 

\begin{center}

  $\frac{\partial f}{\partial x} = \lim_{h \to 0} \left[ \frac{f(x+h, y) - f(x,y)}{h} \right]$
\end{center}


Once you pass in the experimental data, you can pre-process the data, using calculated partial derivatives, for every pair of existing variables. Many physical laws, involve rates of change, and partial
derivatives help us represent them. Furthermore it also guides the search process, as the algorithm can
use the derivative to accurately represent the underlying laws involved. Through comparing how well
the partial derivatives derived through the experimental data compared to the potential expression,
the algorithm can assess the accuracy and feasibility of the expressions involved. This strategy can
even be extended to prune the search space further, this could be achieved through incorporating
knowledge of physics into the constraints for the partial derivatives. These concepts will be illustrated
with an example below.\\


Consider a iron rod, that has been heated up, such that it is hotter on one side than the other. Now it is intuitive to say that closer to the heat source, the temperature will be higher than further along the rod, where it will be colder. We can illustrate this temperature distribution with a function:  \\

\begin{center}
$T(x,y,z)$
\end{center}

where T is the temperature at a point in the rod, and (x,y,z) are the coordinates along the axis in 3 dimensions. This leads to these 3 partial derivatives: \\ 

\begin{center}
 $\frac{\partial T}{\partial x}, \  \frac{\partial T}{\partial y}, \  \frac{\partial T}{\partial z}$
\end{center}

These partial derivatives, gives us information about the direction and magnitude of heat flow at various points on the rod. The algorithm then searches for an equation T(x,y,z), that sufficiently predicts the observed temperature distribution and it's partial derivatives, deriving laws such as the heat transfer equations, or elasticity relationships.\\ 

\begin{center}
$\frac{\partial T}{\partial t} = \alpha \nabla^2 T$
\end{center}



Through using partial derivatives, we have in essence redefined the search criteria for the algorithm, through it's measure of the accuracy in comparison of potential solutions over the invariants represented in the experimental data \cite{Kelly2021} . This also leads to the pleasant finding, that it can additionally capture relationships that represent other identities of the system, beyond invariants and heat transfer equations. \\ 

You can subtly guide the type of laws that such an algorithm finds, by selectively picking the variables to input into the algorithm,. For example providing velocities and force to find laws of motion. \\ 





\begin{center} 
  \textbf {\Large Dimensional Analysis:}
\end{center}

Dimensional Analysis is a method of solving problems usually in maths and physics, where we analyse
the relationships between different physical quantities, by comparing their ”units.” It is a powerful
method of reducing the complexity of systems, enabling engineers and scientists to analyse problems
that we can’t even pose, much less solve the equations of \cite{Longo2021}.\\

Using the fact that numerous questions in science can be simplified by requiring the dimensions/units
of the right and left hand side of the expression to be equal, we can transform the question into
a smaller number of variables, which all have no dimension \cite{Blasiak2012}. It has been automated to find the integer powers of expressions and has proven to be useful especially when the power is an irrational number.\\

Here is a general strategy that showcases how dimensional analysis can be used:\\

Let's say we have a variable in an equation that can be broken down into it's fundamental units, such as (second, kilograms, ampere ...) to various powers. We can then take this, and represent each of the units as vectors, such that each of the fundamental units, is assigned a dimension, and it's important to note, this then allows us to represent any physical quantity as a product of these units, so let us construct a vector $v$, with $3$ integers, where each corresponding integer represents the power of each of the fundamental units.\\ 

Given that we want to derive an expression, such as $y = f(x_1, \dots, x_n)$  we can then create some matrix $M$. Each of the columns of the given matrix, is the unit vector $v$ of the corresponding variable $x_i$. We then need to define another vector to represent the units of y, which will be called $z$. If we let the solution be some vector $s$, solving $Ms = z$, this then lets us raise the powers on both sides, to elevate the independent variables, to make this equation dimensionally consistent.\\ 

Taking the null space of the matrix $M$, where $MV = 0$, allows us a basis to create a dimensionless group, allows for a simplification of the problem.\\

This is also more intuitive to understand physical phenomena, the nature of physics comprehension, making this vital in further understanding derived laws, making the process easier to explain and understand \cite{Taber2009}. Therefore, this is a crucial tool, for cultivating a deeper understanding of physics effectively \cite{Tenachi2023}. \\






\begin{center} 
  \textbf {\Large Genetic Programming:}
\end{center}


Genetic programming (GP), is a special evolutionary algorithmic technique, where the individuals are seen as programs that evolve, starting for a population, is iteratively "evolved," transforming the populations of individual programs, intro other populations. This new generation of programs are created using some genetic operations or survival criteria, mimicking natural evolutionary condition on earth.\\ 

A very basic overview, shows that genetic programming algorithms, consists of initializing the population, then evaluation of the said population through some predefined metrics and functions, followed by selection of the fittest programs based on the score given by the metric, and "genetic operation," such as reproduction, mutation and cross-over. The algorithm then iterates these steps thousands of times, through many generations, and finally terminates once the desired result has been achieved.\\

We can use genetic programming, and tweak the algorithm, and combine it with symbolic regression, to help us derive laws. \\

Consider modelling the various potential formulas as a tree, which is composed of various functions in the nodes. These functions can vary from arithmetic operations  , mathematical functions, or defined unique operators. Then we can program the fitness function \cite{Angeline1994}, and use it to measure how well the given potential expression in the population compares with the given databases, and given the nature of genetic programming, the better performing functions are more likely to be passed down into the next generation. Then after many iterations, we can give the solution with the best performance. \\ 

There are various ways to implement the fitness function, and for example we can use a criteria like this, along with mean squared error \cite{Liddle2009}:\\

\begin{center}
  $V = 2X + N \cdot ln(M/N) $
\end{center}

Here M is the mean squared error, and N is the number of data points, X is the number of parameters used on the genetic programming algorithm. The lower the value of V is, the better the model performs. The performance of this stratergy can then be evaluated with various other metrics, to judge how well the algorithm performs. \\ 




\section{PySR}

This section describes the relevent implementations that are completed as of 10 December 2024.\\ 

\subsection{Momentum Laws: }

To generate the dataset, I chose 100 data points, and created two variables mass (M) and acceleration (a), each represented in two dimensions. Then the data points were generated using $numpy.random.randn$ function. The force (F), was then calculated to be the produce of these two data sets. Mass and acceleration were concatenated along the same axis using $numpy.concatenate$, resulting in a combined dataset. This is partially because the model used here, $PySRRegressor$ expects a single array as input, and this helps highlight the relationship between these variables to the symbolic regression algorithm.\\ 

Then model performed symbolic regression, configured with 40 iterations along with a customer loss function, taken to be the squred diffrence between the prediction and the target variable. \\ 

\begin{center}
  
  $$
  {\boldsymbol{\mathcal{L}(\hat{x}, x) = (\hat{x} - x)^2}}
$$

\end{center}


The model was trained on this dataset, upon termination, it produced a list of potential candidate formulae, from which I manually identified the correct expression, $F = m \dot a$. \\



\begin{algorithm}[H]
\SetAlgoLined
\KwResult{A symbolic representation approximating \( F = M \cdot A \)}
\textbf{Initialization:} \\
Generate random data for mass (\( M \)) and acceleration (\( A \))\;
Compute target force values: \( F = M \cdot A \)\;
Combine \( M \) and \( A \) into input matrix \( X \)\;

\While{Symbolic regression process}{
  Train the symbolic regression model with the following settings:\;
  \textbf{Binary operators:} Multiplication (\(*\))\;
  \textbf{Unary operators:} None\;
  \textbf{Loss function:} Mean squared error between predictions and targets\;
  \textbf{Iterations:} 40\;

  \eIf{Current symbolic representation improves loss}{
   Update the symbolic model\;
   Save the current best expression\;
   }{
   Continue exploration of new symbolic expressions\;
  }
 }
\caption{Symbolic Regression for \( F = M \cdot A \)}
\end{algorithm}

\\
\\

Similarly, the other laws of momentum, were also dervied using this approach. \\ 
\\

\begin{align} \label{eq:impulse_momentum}
\mathbf{F} \Delta t &= \Delta \mathbf{p} = m(\mathbf{v}_f - \mathbf{v}_i) \\
m_1 \mathbf{v}_{1,i} + m_2 \mathbf{v}_{2,i} &= m_1 \mathbf{v}_{1,f} + m_2 \mathbf{v}_{2,f}
\end{align} 
%\subsection{Optimising the Symbolic Regression Algorithm: }

%\subsection{Neural Networks: }

%\subsection{ White Box models: }

\subsection{Pendulum Laws:}

The data is generated using numpy. The simulation involves, Euler's method to solve the pendulum's equation of motion. Through taking small and discrete steps, the method approximates the solution. The equation for a simple pendulum is given by: \\

\begin{center}
\begin{equation}
\alpha = -\frac{g}{L} \sin(\theta)
\end{equation}
\end{center}

\begin{description}
    \item[\(\alpha\)] angular acceleration (\(\text{rad/s}^2\))
    \item[\(g\)] acceleration due to gravity (\(\text{m/s}^2\))
    \item[\(L\)] length of the pendulum (\(\text{m}\))
    \item[\(\theta\)] angular displacement (\(\text{rad}\))
\end{description}

The Euler technique approximates the changes in angular velocity and displacement over some small step in time, as follows:\\

\begin{center}
\begin{align} 
\omega_{i+1} &= \omega_i + \alpha_i \Delta t \\
\theta_{i+1} &= \theta_i + \omega_{i+1} \Delta t 
\end{align}
\end{center}


The function iterates through a few hundred time steps, updating the angular velocity and displacement at each time step. To prevent errors accumulating due to numerical drift, which are small errors that accumulate and become significant due to the inherent nature of approximation methods like Eulers. To keep the values coherent, a wrap around operation is used to ensure the angular displacement is within the range of [\pi, -\pi] radians.\\ 

\subsection{Noise: }

    This section investigates the model's robustness to noise. To simulate varying levels of interference, artificial noise was systematically introduced during the data generation phase, building upon the previously established model framework. Noise was modelled by generating random numbers within a range of progressively increasing magnitude, utilizing Python's random library. The objective was to observe and quantify the degradation in model accuracy as a function of increasing noise levels, as well as explore ways to mitigate it.\\


\subsubsection{How noise affects the model: }

To introduce noise into the generated dataset, I imported Python’s random library and used the randint function. To systematically vary the level of noise, I created an additional function that incrementally increased the parameters passed to randint, causing each successive dataset to become progressively noisier.\\

After generating these noisy datasets, the symbolic regression model was applied to each one, and the resulting equations were analyzed. The relationship between noise level and model performance was then visualized through a graph. Additionally, the time library was used to measure how long each run of the model took.\\

PLOT HERE:\\

\subsubsection{Denoise function: }

    Following the initial noise analysis, a subsequent experiment incorporated a denoising method (implemented using a Python library) applied to the data prior to processing by the pysr model. The results, as depicted in the accompanying plot, indicate that this denoising approach improved model performance up to a specific noise threshold. However, beyond this critical level, performance degraded comparably for both the denoised and non-denoised datasets, suggesting the denoise function's effectiveness diminished at higher noise intensities.\\

PLOT HERE:\\

\section{Symbolic Regression from First Principles: }


\subsection{A Brute-Force Approach: }

The core and essential component of any symbolic regression model lies in its ability to generate and traverse the search space of potential equations and expressions that best fit the given data. To streamline the process and validate the functionality of my expression generation, I began by implementing simple two-variable equations, specifying the operations used within the equation. This approach was initially limited to basic operations, with plans to extend it to accommodate constants and additional complexities.\\

def generate_expressions(variables, constants, operators, max_depth):
    

    symbols = [sp.Symbol(v) for v in variables] + [sp.Integer(c) for c in constants]
    expressions = []
    
    for a, b in permutations(symbols, 2):
        for op in operators:
            try:
                if op == '+':
                    expressions.append(a + b)
                elif op == '-':
                    expressions.append(a - b)
                elif op == '*':
                    expressions.append(a * b)
                elif op == '/':
                    expressions.append(a / b)
                elif op == '**':
                    expressions.append(a ** b)
            except:
                continue
    return expressions


Subsequently, I refined this approach by designing a recursive method to generate expressions, allowing for the creation of more robust and diverse equations from the available variables. This process is dynamic, making it adaptable to a variety of input configurations.\\

def recursive_expressions(operators, variables, constants, max_depth=10):
    symbols = [sp.Symbol(v) for v in variables]

    def build_expressions(current_depth):
        if current_depth == 0:
            return symbols.copy()

        new_expressions = []
        prev_expressions = build_expressions(current_depth - 1)

        for a in prev_expressions:
            for b in prev_expressions:
                for op in operators:
                    try:
                        if op == '+':
                            new_expressions.append(a + b)
                        elif op == '-':
                            new_expressions.append(a - b)
                        elif op == '*':
                            new_expressions.append(a * b)
                        elif op == '/':
                            new_expressions.append(a / b)
                        elif op == '**':
                            new_expressions.append(a ** b)
                    except:
                        continue

        return new_expressions

    all_exprs = build_expressions(max_depth)
    return all_exprs


\subsubsection{ Exploiting Physical Properties: }

The next step involves truncating the generated expressions to prune the search tree as efficiently as possible. One effective method for achieving this is by leveraging the symmetrical properties of physical equations and recognizing their mathematical equivalence. This includes removing redundant or duplicate expressions that do not contribute new information.\\

This is the approach I used to achieve this:\\


\subsubsection{Dealing with constants:}

Another approach I employed to further prune the set of generated expressions was by filtering out any expressions that did not contain all the specified variables. This step helps optimize the process by reducing the number of irrelevant expressions, ultimately saving computation time during the evaluation phase.\\


\subsubsection{Dealing with powers:}

To apply powers to expressions, I incorporated the power operation directly into the generated expressions. This not only allows for the creation of more complex models but also facilitates further pruning of the search tree by avoiding the generation of redundant expressions that already incorporate powers.\\


def apply_powers(expressions, powers, max_depth=2):
    def recursive_powers(expr, depth):
        results = set()
        if depth == 0:
            return {expr}
        
        for power in powers:
            powered = expr ** power
            results.add(powered)

            deeper = recursive_powers(powered, depth - 1)
            results.update(deeper)

        return results

    all_results = set()
    for expr in expressions:
        all_results.add(expr)
        all_results.update(recursive_powers(expr, max_depth))

    return list(all_results)





I implemented a filtering mechanism based on whether the expression included a power operation. This further reduces the size of the search tree by eliminating unnecessary branches. While a more robust model would derive this power operation from scratch, I opted for this approach to optimize computation time and maintain flexibility.\\

def filter_powers(expressions, target_powers):
    filtered = []

    for expr in expressions:
        found = any(
            sub.is_Pow and sub.args[1] in target_powers
            for sub in sp.preorder_traversal(expr)
        )
        if found:
            filtered.append(expr)

    return filtered






\subsubsection{Chaining powers and constants:}

The next step was to chain together powers and constants, applying both to the generated expressions. While the existing design already supports chaining, it is essential to properly filter the results to ensure the search tree remains as compact as possible.\\

Although constants can be filtered using the existing method, the power operations are embedded within the constants, which causes the previous power filter to no longer function as expected. Consequently, I redesigned the filtering process to operate recursively, allowing it to handle both powers and constants effectively.\\


code: \\

However, this approach sometimes results in expressions that feature chained constants, such as sin(sin()). To refine the model further, I introduced an additional filter to remove expressions with multiple instances of the same constant chained together.\\


\subsubsection{Loading data:}

Next, I needed an efficient way to load the data I had created. At this stage, my primary focus was on rapid testing. To facilitate this, I initially generated some dummy data values. Afterward, I decided to store the data as a NumPy array, as this would offer significant speed advantages over using text files. Several factors contribute to this, such as NumPy arrays being stored in memory, the efficiency of the underlying binary data format, and NumPy's use of C, which allows it to vectorize operations, greatly enhancing performance.\\

def load_data(x: np.ndarray, y: np.ndarray, var_names):
    assert x.shape[1] == len(var_names)
    return x, y, [sp.Symbol(v) for v in var_names]



As shown, I perform a check to ensure that the number of variables provided matches the shape of the array X, where X represents the input data, and y represents the target data, i.e., the final result. For example, X contains the mass and acceleration values, while y contains the corresponding values of the force f as calculated by the equation f=maf=ma. This serves as a basic validation to confirm that the number of columns in the input data corresponds correctly to the variables provided.\\

\subsubsection{How to mitigate noise in data: }

Ways to mitigate the noise and it’s affects on the model were explored. Functions such as ”denoise,” in the
symbolic regression library helped to some extent. However after a certain point, such methods do not seem to
offer much assistance.\\
I also made my own denoise algorithm. I implemented various different denoise algorithms to see what effects
they had. Firstly I implemented a simple moving avergae as a way to mitigate the noise in the dataset. reword
this -¿ ” Simple and fast, smooths data well by averaging neighbors. However, it blurs sharp changes and is
sensitive to extreme outlier values, pulling the average significantly and distorting the signal.”
These were my results, this is the pseudo code, explain the alogrithm\\

The second denoise algoirthm I implemented is a median filter, and this is what effects it has, and this is how
i implemented it. Insert Pseudo code. reword: ”Excellent at removing spikes and preserving edges better than
averaging. Less affected by outliers. Can sometimes slightly distort the overall shape of the signal, especially
with large window sizes.”\\

Finally this is the third algorithm that I had implemented for denoising. Wavelet Denoising, this is the effects,
and this is the pesudo code. Reword this -¿ ”Transforms data to isolate noise, preserving both smooth and sharp
signal features effectively. More complex to understand and requires careful selection of wavelet type and pa-
rameters for optimal results, which can be tricky.”\\





\subsubsection{Evaluating expressions:}


Next, I evaluate the expressions that have been generated. I assign the input variables to the corresponding columns of the data in increasing order. These values are then substituted into the expressions, and the model runs the calculations, producing an array of outputs for each expression. This process essentially evaluates every pruned expression and returns a NumPy array of results based on the input data.\\

def evaluate_expression(expression, variables, X): 
    
    symbols = [sp.Symbol(v) for v in variables]
    func = sp.lambdify(symbols, expression, modules='numpy')
    try:
        inputs = [X[:, i] for i in range(X.shape[1])]
        return func(*inputs)
    except Exception as e:
        return np.full(X.shape[0], np.nan)
    


Following the approach outlined in the paper [insert citation here], I utilized a medium error description length loss function, implementing it as described. The error is calculated using the squared difference to ensure all errors are positive, and a constant of 1 is added to guarantee that all errors are greater than 1 when taking the logarithm.\\


def mean_error_descripton_length(result, original):
    result = np.array(result).flatten()
    original = np.array(original).flatten()

    total_log_error = 0.0
    n = len(result)

    for i in range(n):
        error = abs(original[i] - result[i])
        log_error = np.log2(1 + error ** 2)
        total_log_error += log_error
        print(f"Index {i}: true={original[i]}, pred={result[i]}, error={error}, log_error={log_error:.4f}")

    return (total_log_error / n)






\subsection{Polynomial Fit Module: }

Now that the core of the algorithm is functional—handling constants, powers, variables, generating expressions, and filtering redundancy using physical principles like symmetry—I aimed to extend the program by implementing a polynomial fitting module. The goal of this technique is to efficiently fit data to a polynomial model, as many functions in physics (or parts of them) are well-approximated by low-order polynomials, and polynomial fitting is a computationally inexpensive method for this specific class of functions. The technique generates all possible polynomial terms up to a specified degree (e.g., degree 4) and creates a linear equation for each data point where the unknowns are the polynomial coefficients. The system of equations is solved using standard methods such as least squares, and the Root Mean Squared Error (RMSE) of the fit is calculated. If the RMSE is below a predefined tolerance (denoted as pol), the polynomial is accepted as a solution. This approach serves as a fast base case in the recursive algorithm, quickly solving problems that are simple polynomials, and it can also handle sub-problems transformed into polynomial form by other modules, such as dimensional analysis or function inversion.\\


\subsubsection{Data Loading:}

To begin, I developed the data loading function. The goal was to accept a NumPy array containing the data, along with a list of variables. The function then compares the shape of the data array with the number of variables provided to ensure that the input is consistent and sufficient for further processing.\\


def load_data(data_array, variables):

    num_cols = data_array.shape[1]
    num_vars = len(variables)

    if num_cols != num_vars:
        raise ValueError(f"Data has {num_data_cols} columns but {num_vars} variables were provided.")

    return data_array




\subsubsection{Generating polynomial expressions:}

The next step involves generating polynomial expressions. The function returns a list of polynomial expressions based on the input coefficients, variables, and operators, considering a specified maximum degree for the terms.\\

The function works by first creating symbolic representations for the variables. It then iterates over all possible combinations of powers for the variables up to the specified degree and combines these terms using the provided operators. Finally, the generated expressions are simplified and returned as a list.\\


def generate_expressions(coeffs, variables, operators, max_degree):

    vars_syms = [sp.Symbol(v) for v in variables]

    expressions = []


    for powers in itertools.product(range(1, max_degree + 1), repeat=len(vars_syms)):
        terms = [c * (v ** p) for c, v, p in zip(coeffs, vars_syms, powers)]
        for ops in itertools.product(operators, repeat=len(terms) - 1):
            expr = terms[0]
            for op, term in zip(ops, terms[1:]):
                if op == '+':
                    expr = expr + term
                elif op == '-':
                    expr = expr - term
                elif op == '*':
                    expr = expr * term
                elif op == '/':
                    expr = expr / term
            expressions.append(sp.simplify(expr))

    return expressions




\subsubsection{Filtering the Polynomial expressions:}

The filter_expressions function programmatically filters symbolic expressions based on both structural and semantic constraints. It is particularly suited for large-scale symbolic filtering tasks where strict mathematical structures must be enforced.\\

The initial version of this function worked for symbolic constants (e.g., sin, cos, etc.) but failed to handle numbers or integer coefficients. This issue was identified during testing, prompting me to rewrite the function so that it could also handle integer coefficients properly.\\

def filter_expressions(expressions, required_vars, required_constants, required_power):
    result = []

    required_vars = {sp.Symbol(v) for v in required_vars}

    for expr in expressions:
        symbols_ok = required_vars.issubset(expr.free_symbols)

        if not all(
            any(
                (isinstance(sub, const) if isinstance(const, type)
                 else sub == const)
                for sub in sp.preorder_traversal(expr)
            )
            for const in required_constants
        ):
            continue


        power_ok = any(
            isinstance(sub, sp.Pow) and sub.exp == required_power
            for sub in sp.preorder_traversal(expr)
        )

        if symbols_ok and power_ok:
            result.append(expr)

    return result



\subsubsection{Evaluating expressions:}

The next step involves fitting the filtered expressions to the dataset. The model fitting function fits polynomial expressions to the input data by determining the optimal set of coefficients that minimize the error between the predicted and actual output values. It evaluates multiple polynomial degrees, up to a specified maximum, and selects the degree that results in the lowest error, thereby ensuring an optimal balance between accuracy and complexity.\\

I utilize the Root Mean Squared Error (RMSE) to calculate the loss, and the function returns a list of loss values, one for each expression.\\

def evaluate_expressions(expressions, variables, data, y_true):
    results = []
    for expr in expressions:
        try:
            func = sp.lambdify(variables, expr, modules='numpy')
            y_pred = np.array([func(*row) for row in data])
            rmse = np.sqrt(np.mean((y_pred - y_true)**2))
            results.append((expr, rmse))
        except Exception as e:
            print(f"Skipping {expr} due to error: {e}")
    return results

To begin the fitting process, I take an expression, substitute the input variables with the corresponding values from the dataset, and compute the predicted yy-values based on the equation. I then calculate the difference between the predicted values and the true target values (yy), which are the actual outputs. This difference is used to compute the Root Mean Squared Error (RMSE), which quantifies the prediction error for the expression.\\

\subsubsection{Best Polynomial Fit:}

After calculating the RMSE values for each expression, I select the expression with the lowest RMSE as the most accurate polynomial fit for the data. This ensures that the chosen model has the best performance in terms of minimizing prediction error.\\

def bestFit(results):
    return min(results, key=lambda x: x[1])



\subsection{Dimensional Analysis: }


Physical equations must be dimensionally consistent, meaning the units on both sides of the equation must match, which severely limits the possible forms of the unknown function. This dimensional constraint provides a strong simplification of the problem, significantly narrowing the scope of valid equations. AI Feynman addresses this by applying dimensional analysis as the first step, simplifying the problem by identifying which combinations of variables are dimensionally consistent. The units of the variables—such as mass, length, and time—are represented as vectors of integer powers, forming a linear system based on the unit vectors of the input and target variables. Solving this system and finding the null space reveals dimensionless combinations of variables, which transforms the problem into one of finding a function of these new dimensionless variables. This process typically reduces the number of independent variables that the algorithm needs to search over, drastically shrinking the combinatorial search space for subsequent steps, such as polynomial fitting, brute force, and neural network-guided searches. As a result, the reduction in variables leads to a significant boost in efficiency, making these searches faster and more likely to succeed.\\


\subsubsection{Handling Units:}


The AI Feynman database was accessed, and the units.csv file was downloaded to better understand the units present in the dataset. Upon reviewing the required units, a unit table was created in the form of an array, where each unit corresponds to a unique power of the fundamental SI units. Additionally, the basic SI units were implemented as an array/list to facilitate this mapping.\\


UNIT_TABLE = {

    'mass':                 [1, 0, 0, 0, 0, 0, 0], 
    'length':               [0, 1, 0, 0, 0, 0, 0],
    'time':                 [0, 0, 1, 0, 0, 0, 0],
    'temperature':          [0, 0, 0, 1, 0, 0, 0],
    'current':              [0, 0, 0, 0, 1, 0, 0],
    'amount':               [0, 0, 0, 0, 0, 1, 0], 
    'luminous_intensity':   [0, 0, 0, 0, 0, 0, 1],


There were also relevent derived units included.\\ 

\subsubsection{Construct Matrix and Target Vector:}

This function constructs the dimensional matrix MM and the target vector bb, which are essential for performing dimensional analysis. It accepts lists of independent and dependent variable names, along with a dictionary that maps each variable name (as the key) to its corresponding unit vector (as the value). The unit vectors for the independent variables are retrieved through dictionary lookup, using lowercase variable names (i.e., var.lower()) to ensure case-insensitivity. These vectors are then efficiently assembled into the columns of matrix MM using numpy.column_stack, while the unit vector of the dependent variable forms the target vector bb. This approach ensures usability through case-insensitivity, leverages the performance benefits of numpy.column_stack, and includes explicit error handling to prevent issues arising from missing or incorrect keys.\\





\subsubsection{Solving Dimension and Basis Units:}

This function determines the exponents for dimensional scaling and dimensionless groups by solving the lin-
ear systems Mp = b and MU = 0. It takes the dimensional matrix M and target vector b as input. The
function first converts these NumPy arrays into SymPy matrices (sp.Matrix) to leverage symbolic compu-
tation capabilities. It then attempts to find an exact particular solution p for Mp = b using SymPy’s LU-
solve method, chosen for its ability to yield rational solutions. Robust error handling via try...except ad-
dresses potential issues like inconsistent systems. Finally, it calculates the null space basis U of M using
Ms ym.nullspace(), whichidentif iesthecombinationsf ormingdimensionlessgroups.T hef unctionreturnsthesymbolicsolu
Think of it like this: you have a target physical quantity (b, like Force) that depends on several input quantities
(M, like mass, length, time).
Finding the ”Unit-Fixing” Part (p = Ms ym.LU solve(bs ym)) :
The function first figures out the specific combination of powers (p) of your input variables (x) that you need to
multiply together ( xp )sothattheresulthastheexactsamephysicalunitsasyourtargetvariable(b).
For example, if the target is Force ([M L T2]) and inputs are mass ([M]), length ([L]), and time ([T]), it would
find p corresponding to mass1 * length1 * time2.
It uses LUsolve from SymPy to try and find an exact (often simple fraction or integer) solution for these powers
p.
Finding the ”Dimensionless Combinations” (U = Ms ym.nullspace()) :
After accounting for the basic units, any remaining relationship must involve combinations of input variables
that have no units at all (they are dimensionless numbers, like Reynolds number).
The function finds all the fundamental ways (U) you can combine powers of the input variables ( xu )suchthattheunitscompletelycan
In essence, the function:
Separates the part of the formula responsible for getting the units right (p).
Identifies all the core dimensionless building blocks (U) that the rest of the formula must be made from.
This allows the main algorithm to later focus on finding the relationship between these dimensionless quantities,
which is a simpler problem than the original one involving various physical units. 



\subsubsection{Data Transformation Function:}
his function converts original physical data (datax , datay )intoadimensionlessf ormusingpreviouslycalculatedexponents(p, U ).
How it Works: First, it calculates a scalingf actorf oreachdatapointbyraisinginputvariables(datax )tothepowersspecif iedinp(u\\



\subsubsection{Symbolic Transformation Generator: }

This function generates the symbolic mathematical expressions corresponding to the dimensional analysis trans-
formation. It accepts the original independent variable names (independentv ars)andtheexponentvectorsf orscaling(p)anddimens
How it Works: It first creates SymPy symbolic objects for each input variable name using sp.symbols. Utilizing
these symbols and the scaling exponents p, it constructs the symbolic expression symbolicp representingtheunit−
f ixingscalingf actor(xp )viasp.M ul.Subsequently, ititeratesthrougheachexponentvectoruinU, buildingthecorrespondings
insert code:\\


\subsection{Neural Network Fitting: }

The next step crutical component, is using neural networks in order to simplify the expressions further.
In order to get a smotth and differentiable approximation of the function we are looking for, this is what the
neural network allows us to do.
The neural network provides a powerful, universal function approximator (fN N )capableof learningintricatepatternsdirectlyf random\\


\subsubsection{SymbolicNetwork:}

This class defines the neural network architecture used as a universal function approximator within the sym-
bolic regression framework. It inherits from torch.nn.Module, the base class for all neural network modules in
PyTorch.
Technical Implementation: The constructor (i nit) initializesthenetworkstructure,acceptingthenumberof inputf eatures(ni nput)anddef aultingtooneoutpu
Design Rationale: This multi-layer perceptron (MLP) architecture provides significant expressive power. The
Tanh activation was chosen as specified in the reference papers, offering a smooth, differentiable non-linearity
suitable for approximating complex physical functions and enabling reliable gradient computation for subse-
quent analysis steps.\\ 


\subsubsection{Preparing the data:}

This function preprocesses raw input and output data (datax , datay )intoaf ormatsuitablef orP yT orchmodeltrainingandvalidati
Technical Implementation: It begins by converting the input NumPy arrays datax anddatay intoP yT orchtensorsof typetorch.f loat3
Design Rationale: This design adheres to standard PyTorch practices. Tensor conversion is mandatory for
framework compatibility. The train/validation split is critical for monitoring model generalization and prevent-
ing overfitting. TensorDataset provides a convenient pairing of inputs and targets. DataLoader is essential for
efficient training, enabling batch processing (managing memory and potentially parallelizing computation) and
automating data shuffling, which improves model robustness and convergence.\\

\subsubsection{Training the Network:}

This function orchestrates the supervised training process for the provided PyTorch neural network model. Its
primary goal is to iteratively adjust the model’s parameters (weights and biases) to minimize the difference
between its predictions and the true target values using the training data, while monitoring performance on
unseen validation data.
Technical Implementation: The function begins by transferring the model to the specified compute device (’cpu’
or ’cuda’). It initializes the Adam optimizer, a common adaptive learning rate optimization algorithm, linking it
to the model.parameters() and setting the learningr ate.T heM eanSquaredErrorloss(nn.M SELoss), suitablef orregression, isc
Design Rationale: This structure represents a standard PyTorch training loop. Using DataLoader enables effi-
cient batch processing. The Adam optimizer provides robust convergence properties. MSE loss directly min-
imizes the squared prediction error. Explicitly setting model.train() and model.eval() ensures correct behavior of
layers like dropout or batch normalization (if present). The torch.nog rad()contextduringvalidationpreventsunnecessarygradien\\


\subsubsection{Predict Function:}

This function performs inference using a trained PyTorch model, generating output predictions for a given set of
input data. It is designed to take a trained model, input data as a NumPy array (xn umpy), andthetargetcomputationdevice(′ cpu′ or′
Technical Implementation: The function first sets the model to evaluation mode using model.eval(). This is cru-
cial as it disables layers like dropout or batch normalization that have different behaviors during training and in-
ference, ensuring deterministic output. The input NumPy array xn umpyisthenconvertedintoatorch.T ensorwithdtype =
torch.f loat32andtransf erredtothespecif ieddevice.T hecorepredictionstepoccurswithinawithtorch.nog rad() :
contextmanager.T hisdisablesgradientcalculation, signif icantlyreducingmemoryconsumptionandspeedingupcomputatio
Design Rationale: This design follows standard PyTorch inference practices. model.eval() ensures correct
prediction behavior. Tensor conversion and device handling manage data compatibility with the model. The torch.nog rad()contextoptimizesperf ormancef orinf erence.ReturningaN umP yarrayprovidesauser−
f riendlyoutputf ormat.\\ 


\subsection{Pareto Frontier Optimisation: }

The Pareto frontier provides a principled way to manage the inherent trade-off between a formula’s accuracy
and its simplicity (complexity) in symbolic regression. Instead of seeking a single ”best” formula, the goal is
to identify all Pareto-optimal formulas: those for which no other discovered formula is simultaneously simpler
and more accurate.
I am going to implement this by plotting on a 2d plane, where x represent complexity and y represents the mean
error description length. As the algorithm generates candidate formulas (through brute-force search, trans-
formations, or recursive combinations), each candidate is evaluated and potentially added to the set of points
forming the frontier.
Crucially, any candidate formula that is dominated – meaning another formula on the frontier has both lower (or
equal) complexity and lower (or equal) loss (with at least one inequality strict) – is discarded. This Pareto prun-
ing significantly reduces the search space, especially when combining solutions from subproblems, improving
computational efficiency and robustness against noise by inherently favouring simpler explanations for a given
accuracy level. The final frontier presents the user with a spectrum of optimal solutions.\\


\subsubsection{Points:}

This Point class serves as a fundamental data structure within the Pareto frontier analysis framework. Its pri-
mary purpose is to encapsulate the essential characteristics of a single candidate formula evaluated during the
symbolic regression process.
Technical Implementation: The constructor (i nit initializeseachP ointinstancewiththreeattributes:complexity(anumericalscorerepresentingthef ormula
)
Design Rationale: This class is designed to bundle the key metrics (complexity, accuracy) required for Pareto
dominance checks with the associated formula (expression). By grouping these related pieces of data into a
single object, it simplifies the management and comparison of candidate solutions. Functions operating on
the Pareto frontier can easily access the necessary complexity and accuracy attributes from each Point object to
determine if one solution dominates another, thereby facilitating the efficient maintenance of the non-dominated
set of optimal formulas.\\


\subsubsection{Pareto Point Set:}

The updatep aretop ointsf unctionmaintainsaP aretof rontierof symbolicexpressions, balancingcomplexity(modelsimplicit
The function filters the combined list to retain only non-dominated points: a point A dominates point B if A is
both no more complex and no less accurate, with at least one being strictly better. This ensures only the best
trade-offs remain.
In symbolic regression, this is critical because we aim to discover expressions that are not just accurate but
also interpretable — meaning low complexity. The Pareto frontier lets us visualize and choose among optimal
models, avoiding overfitting (too complex) or underfitting (too simple).\\ 


\subsection{Plotting: }

I wanted ways to visualise and understand in a visual sense what was happening behind the scenes in my sym-
bolic regression program. Humans are visual creatures and looking at plots offers a more powerful way to understand data rather than staring a strng of numbers or expressions, trying to manually find patterns or mis-
takes.
I decided to use plotly, instead of the usual matplotlib, as I personally think it is more asthetically pleasing and
it has a better interactive environment.\\ 


\subsubsection{Plot for Pareto front:}

Visualizing the Pareto frontier is highly beneficial for interpreting the results of symbolic regression. It provides
an intuitive graphical representation of the fundamental trade-off between model complexity and predictive
accuracy (or loss).
Code Description: The provided code utilizes the plotly.express library to generate this visualization. A Pandas
DataFrame (df) structures the data, holding columns for ’Complexity’ (x-axis), ’Loss’ (y-axis, representing inac-
curacy like MEDL), and the corresponding symbolic ’Expression’. The px.scatter function creates a scatter plot
mapping complexity against loss, crucially using the ’Expression’ data to label each point directly on the plot via
the text argument. Further customization using fig.updatet racesenhancespointvisibility(markersize, color)andlabelpositioning
Utility and Rationale: This visualization allows researchers to readily identify the set of non-dominated solu-
tions. By plotting quantitative metrics, it elucidates points where significant gains in accuracy require substantial
increases in complexity (an ”elbow” region), facilitating informed model selection based on the specific bal-
ance desired between interpretability/simplicity and predictive power. The direct labeling of points with their
expressions provides immediate context for each optimal solution discovered.\\ 

\subsection{Main method bringing it together Regressor: }

\subsubsection{main solver}

Part 1: Dimensional Analysis and Symbolic Scaling

The function first applies Dimensional Analysis (DA) to reduce the problem’s dimensionality. It computes a transformation matrix and scaling vector by solving the unit balance equation between independent and target variables. If no dimensionless groups are found, the result is a simple scaling law expressed symbolically. Otherwise, it computes and displays the dimensionless groups and the scaling part of the solution. This ensures the model operates on physically meaningful, unit-consistent quantities, simplifying the functional search space and preventing non-physical relationships. If DA completely solves the problem, the function terminates here by outputting the symbolic solution.
Part 2: Neural Network Approximation of Dimensionless Relation

When dimensionless groups exist, the function generates dimensionless data and fits a lightweight symbolic neural network to model the relationship between transformed inputs and outputs. A standard training pipeline is executed: data preparation, model instantiation, training with backpropagation, and prediction. The model’s mean squared error (MSE) is computed on the dimensionless targets to evaluate fit quality. Neural network outputs (predictions and gradients) serve as a flexible preliminary approximation, potentially capturing nonlinear relations that guide the subsequent, more rigid symbolic regression steps.
Part 3: Polynomial Candidate Generation and Filtering

Following the neural fit, the function applies Polynomial Fitting (PF) techniques to generate candidate symbolic expressions. Polynomial terms are systematically composed using provided variables, coefficients, and operators up to a specified degree. Generated expressions are filtered based on structural criteria, such as variable presence and coefficient validity, to ensure physical plausibility. Each polynomial is evaluated against the original data, and a best-fit candidate is selected according to its error score. If a candidate perfectly matches (zero error), the function outputs the discovered symbolic expression and terminates.
Part 4: Brute Force Symbolic Search and Evaluation

If no polynomial yields a perfect fit, the function initiates a Brute Force (BF) symbolic search. It exhaustively generates expressions by combining variables, constants, operators, and powers. Multiple filters (symmetry, variable relevance, power range, constant handling) systematically prune the expression set to reduce computational complexity and preserve physically meaningful candidates. The remaining expressions are evaluated against the dataset, seeking the best match based on performance metrics. This final exhaustive step ensures that even highly non-obvious symbolic relations can be discovered if they exist within the defined operator and degree space.

\subsection{Testing:}
Every module, function and file, were thoroughly tested using dedicated tests. The boundary conditions and
inserted tests to make sure the function behaved as envisioned. There was also significant robust testing func-
tions written when chaining together various techniques and models, in order to ensure everything was working
smoothly.\\

\subsubsection{Unit Testing: }

I've written unit tests for every function, every evaluation method and have rigorously tested their functionality, edge cases, normal inputs etc. \\

Rigorous unit testing was employed to validate the correctness of each module within the symbolic regression framework. Key components including Pareto frontier operations, polynomial generation and evaluation, and dimensional analysis routines were individually tested using structured unit tests. Each test assessed expected outputs under standard and edge-case inputs.

Test results confirmed correct behavior, and outputs aligned with theoretical expectations (e.g., correct Pareto set retrieval, successful polynomial fitting on synthetic data, and dimensionally consistent variable transformations). A summary of testing outcomes is shown below. Furthermore, synthetic datasets based on known biological formulas were used to validate the regressor’s ability to recover exact symbolic relationships, demonstrating the system’s practical reliability.\\



% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\textit{\textbf{Module:}} & \textit{\textbf{Tests:}}                   & \textit{\textbf{Results:}}    \\ \hline
Dimensional Analysis & Get, Solve, Generate, Symbolic Transformation                                    & {\color[HTML]{000000} Passed} \\ \hline
Neural Network       & Load, Network, Train, Predict, Gradient                                          & {\color[HTML]{000000} Passed} \\ \hline
Plotting                  & Pareto, Gradient                           & {\color[HTML]{000000} Passed} \\ \hline
Polynomial Regressor      & Load, Generate, Filter, Evaluate, Best fit & Passed                        \\ \hline
Brute Force          & Load, Generate, Recursive, Evaluate, Loss, Variable Filtering, Constants, Powers & Passed                        \\ \hline
Main Solver               & Main Solver, Laws                          & Passed                        \\ \hline
\end{tabular}%
}
\end{table}



\subsubsection{Integration Testing: }


To validate the interoperability and functional correctness of the developed software system, a comprehensive suite of integration tests was designed and executed. These tests focused on the interactions between the core Python modules, including Dimensional Analysis (dimensionalAnalysis), Polynomial Fitting (polynomialFit), Brute-Force Symbolic Regression (bruteForce), Neural Network (neuralNetwork) components, Pareto front optimization (pareto), and visualization (plots). Utilizing Python's unittest framework, the tests simulated realistic data analysis workflows, verifying the seamless flow of data and control between modules.\\

Key tested interactions included the preprocessing of data using Dimensional Analysis feeding into various model fitting approaches (PF, BF, NN), the evaluation of candidate expressions generated by BF and PF modules and their subsequent management on a Pareto front, and the symbolic transformation capabilities of the Dimensional Analysis module. Further tests confirmed the internal consistency of complex operations within the Brute-Force module and the functionality of gradient calculations in the Neural Network. Successful execution of these tests demonstrates the robustness of the integrated system and its capability to perform complex, multi-stage symbolic regression tasks as designed, ensuring reliable data handoffs and component communication within the software architecture.\\


\subsubsection{Performance Testing: }

To quantitatively evaluate the computational efficiency and scalability of the core symbolic regression components, dedicated performance tests were executed on the bruteForce and polynomialFit modules. Utilizing Python scripts leveraging the time module for execution duration measurement and the psutil library for monitoring Resident Set Size (RSS) memory usage, these tests targeted the primary computational bottlenecks: symbolic expression generation and expression evaluation against numerical data.\\

The methodology involved systematically varying key parameters influencing complexity, such as expression generation depth (bruteForce), maximum polynomial degree (polynomialFit), the number of input variables, and the size of the input data sample for evaluation. By recording execution time and memory consumption under these varying conditions, the tests aimed to characterize the performance scaling of each module's algorithms. This analysis provides crucial insights into the computational complexity and resource requirements associated with each approach, enabling an understanding of their practical limitations and scalability characteristics. The results quantify the performance trade-offs inherent in different symbolic search strategies and inform the feasibility of applying the developed system to large-scale scientific discovery tasks by establishing empirical benchmarks for time and memory demands.\\


\subsubsection{AI-Feynman Dataset: }


\section{ Applying the model to Biological Data: }

However, its application to biological systems remains an underexplored frontier. Biological processes are inherently noisy, high-dimensional, and complex, often lacking explicit governing equations. As a result, there exists a significant research gap in employing symbolic regression techniques for biological data modeling. A limited number of studies attempt this integration, making it a novel and potentially revolutionary field of study.\\

In this work, we apply symbolic regression to a fundamental biological process: the prediction of nucleic acid melting temperature (Tm) from DNA/RNA base counts. The melting temperature is computed through a well-established empirical formula known as the Wallace rule, defined as:\\
Tm=2(A+T)+4(G+C)\\
Tm=2(A+T)+4(G+C)\\

where A,T,G,CA,T,G,C denote the counts of adenine, thymine, guanine, and cytosine bases, respectively.\\

To test the approach, synthetic data is generated programmatically. Arrays are constructed where each sample consists of four integer inputs corresponding to base counts. The target yy values are computed exactly according to the Wallace formula. Variables (['A', 'T', 'G', 'C']) and constants ([2, 4]) are explicitly defined to align with the biological context.\\

The symbolic regression system first performs dimensional analysis, even though biological units are less rigorously defined compared to physics. Then, using a neural-symbolic approach followed by polynomial and brute-force search stages, the system attempts to recover the original expression.\\

This study highlights how biological systems can be rendered into symbolic, mechanistic expressions, bridging the gap between empirical observation and interpretable models, and opening new avenues for data-driven biological discovery.\\


\section{Evaluation:}

\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{\jobname} 


\addcontentsline{toc}{section}{Appendix:}

\section{Project Planning: }


\end{document}
