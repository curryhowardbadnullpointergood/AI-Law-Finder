

  The main questions I have are related to notes page, as well as the mark scheme:

  1. Is the second examiner chosen based on the project brief, so they're picked if the project is related to their research, or is it random? 

  2. I might need to potentially test my shogi A.I against human players, do I need their "concent," to gather data? I need to submit a form to the resear ethics board, for permission to use past human data (previous games played by humans), as well as potentially testing this against humans. 

  3. How do I apply for this, and getting humans to play against my A.I may be difficult if I need them to fill out a form before they play. 

  4. Can I contact professionals in the game, and try get them to play against my finished A.I, to test it's effectiveness in a real world setting?

5. " practical uses of the project, a student who addresses the practical uses of their project in the final report will probably do better than one who doesn't. " - so by this, does this mean any practical uses, or commerical uses are awarded more marks? Where does this leave potentially purely theoretical projects, ie making/testing a new method of game evaluation that may say perform worse than it's traditionally counterpart, but is innovative? 

  6. What sort of citation and referencing format would you recommend? 
:wq

  7. Is the project brief something set in stone, or is there potential to slightly change it in the future?  

  8. Category "A," in the markscheme, mentiones that it is about 

    " Achievement, innovation, creativity, challenge, research contribution,"
    
    Innovation, is pretty easy to understand, however the other metrics seem subkective, ie what is challenging? 

    So I'd like to know how "challenging," my project seems, as I don't want 
    to cap my potential marks. What defines creative? I plan on useing pretty 
    challenging yet straighforward algorithms, is the creative aspect then measuring for example how it is put together? By "Achievement," is this measuring the effectiveness of the program (ie: elo rating of my A.I. ?) 

    I'm confused what research collaboration means, pretty sure it's plagarism if I collaborate with other people.

    What exactly is does "new and publishable," results mean? You are probably
    aware most papers published are just someone else's idea taken and made 
    slightly different. is this level of "new," appropriate, if not something
    completely new, isn't feasible and therefore I can expect marks to be hard
    capped at 89%. If not are there any example of thongs considered "new"?  




    9. Category "T," is a bit ambiguous, what classifies as outstanding analysis, vs required analysis? Again "extensive range of tools," seems nteresting, as it's not a solid number it's hard to guage what to do here, there's plently of tools and trying to "fit," them in probably isn't a good idea, ie it's good to use 1 good tool than 5 bad ones.

    10. What sort of questions will be asked in the Viva, will it be related to my project or general A.I. questions? Is it possible to get a sample of projects and questions asked previously, so I understand what I should expect. 
