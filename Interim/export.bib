@inbook{Liddle2009,
   abstract = {Introduction One of the principal aims of cosmology is to identify the correct cosmological model, able to explain the available high-quality data. Determining the best model is a two-stage process. First, we must identify the set of parameters that we will allow to vary in seeking to fit the observations. As part of this process we need also to fix the allowable (prior) ranges that these parameters might take, most generally by providing a probability density function in the N-dimensional parameter space. This combination of parameter set and prior distribution is what we will call a model, and it should make calculable predictions for the quantities we are going to measure. Having chosen the model, the second stage is to determine, from the observations, the ranges of values of the parameters which are compatible with the data. This second step, parameter estimation, is described in the cosmological context by Lewis and Bridle in Chapter 3 of this volume. In this article, we shall concentrate on the choice of model. Typically, there is not a single model that we wish to fit to the data. Rather, the aim of obtaining the data is to choose between competing models, where different physical processes may be responsible for the observed outcome. This is the statistical problem of model comparison, or model selection. This is readily carried out by extending the Bayesian parameter estimation framework so that we assign probabilities to models, as well as to parameter values within those models.},
   author = {Andrew R. Liddle and Pia Mukherjee and David Parkinson},
   doi = {10.1017/CBO9780511802461.005},
   journal = {Bayesian Methods in Cosmology},
   title = {Model selection and multi-model inference},
   volume = {9780521887946},
   year = {2009},
   publisher = {online},
   chapter = {2},
}
@article{Angeline1994,
   abstract = {Genetic programming may be more powerful than neural networks and other machine learning techniques, able to solve problems in a wider range of disciplines. In this ground-breaking book, John Koza shows how this remarkable paradigm works and provides substantial empirical evidence that solutions to a great variety of problems from many different fields can be found by genetically breeding populations of computer programs. Genetic Programming contains a great many worked examples and includes a sample computer code that will allow readers to run their own programs.In getting computers to solve problems without being explicitly programmed, Koza stresses two points: that seemingly different problems from a variety of fields can be reformulated as problems of program induction, and that the recently developed genetic programming paradigm provides a way to search the space of possible computer programs for a highly fit individual computer program to solve the problems of program induction. Good programs are found by evolving them in a computer against a fitness measure instead of by sitting down and writing them.John R. Koza is Consulting Associate Professor in the Computer Science Department at Stanford University.},
   author = {Peter J. Angeline},
   doi = {10.1016/0303-2647(94)90062-0},
   issn = {03032647},
   issue = {1},
   journal = {Biosystems},
   title = {Genetic programming: On the programming of computers by means of natural selection,},
   volume = {33},
   year = {1994},
}
@article{Tenachi2023,
   abstract = { Symbolic regression (SR) is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, the development of SR methods has not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present Φ-SO, a physical symbolic optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions but also because the grammatical rules of dimensional analysis enormously restrict the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful, for instance, when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations of noisy data. We test our machinery on a standard benchmark of equations from the Feynman Lectures on Physics and other physics textbooks, achieving state-of-the-art performance in the presence of noise (exceeding 0.1%) and show that it is robust even in the presence of substantial (10%) noise. We showcase its abilities on a panel of examples from astrophysics. },
   author = {Wassim Tenachi and Rodrigo Ibata and Foivos I. Diakogiannis},
   doi = {10.3847/1538-4357/ad014c},
   issn = {0004-637X},
   issue = {2},
   journal = {The Astrophysical Journal},
   title = {Deep Symbolic Regression for Physics Guided by Units Constraints: Toward the Automated Discovery of Physical Laws},
   volume = {959},
   year = {2023},
}
@article{Taber2009,
   author = {Keith S. Taber},
   doi = {10.1088/0031-9120/44/4/F01},
   issn = {00319120},
   issue = {4},
   journal = {Physics Education},
   title = {Maths should be the last thing we teach},
   volume = {44},
   year = {2009},
}
@article{Blasiak2012,
   abstract = {The paper presents the results of research on the relationship between self-assessed comprehension of physics lectures and final grades of junior high school students (aged 1315), high school students (aged 1618) and physics students at the Pedagogical University of Cracow, Poland (aged 21). Students' declared level of comprehension was measured during a physics lecture on a prearranged scale of 110 with the use of a personal response system designed for the purpose of this experiment. Through the use of this tool, we obtained about 2000 computer records of students' declared comprehension of a 45 min lecture, which we named the spectrum of comprehension. In this paper, we present and analyse the correlation between students' declared comprehension of the content presented in the lecture and their final learning results. © 2012 IOP Publishing Ltd.},
   author = {W. Blasiak and M. Godlewska and R. Rosiek and D. Wcislo},
   doi = {10.1088/0143-0807/33/3/565},
   issn = {13616404},
   issue = {3},
   journal = {European Journal of Physics},
   title = {Spectrum of physics comprehension},
   volume = {33},
   year = {2012},
}
@inbook{Longo2021,
   abstract = {The expression of the equations describing the physical phenomena requires the respect of some formal rules and the knowledge of some basic principles that guarantee correctness and logical coherence. In particular, the structure of an equation that connects physical quantities must respect the principle of dimensional homogeneity.},
   author = {Sandro G. Longo},
   doi = {10.1007/978-3-030-79217-6_1},
   issn = {21924740},
   journal = {Mathematical Engineering},
   title = {Dimensional Analysis},
   year = {2021},
   publisher = {online},
   volume = {05},
   chapter = {5}, 
}
@article{Kelly2021,
   abstract = {Partial differential equations form an essential part of the core mathematics syllabus for undergraduate scientists and engineers. The origins and applications of such equations occur in a variety of different fields, ranging from fluid dynamics, electromagnetism, heat conduction and diffusion, to quantum mechanics, wave propagation and general relativity.This volume introduces the important methods used in the solution of partial differential equations. Written primarily for second-year and final-year students taking physics and engineering courses, it will also be of value to mathematicians studying mathematical methods as part of their course. The text, which assumes only that the reader has followed a good basic first-year ancillary mathematics course, is self-contained and is an unabridged republication of the third edition published by Longman in 1985.},
   author = {G. V. Kelly},
   doi = {10.33232/bims.0018.94.96},
   issn = {0791-5578},
   journal = {Irish Mathematical Society Bulletin},
   title = {Partial Differential Equations for Scientists and Engineers by G. Stephenson},
   volume = {0018},
   year = {2021},
}
@article{Smith2012,
   abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
   author = {R.T. Smith},
   issn = {1098-6596},
   issue = {9},
   journal = {Calculus Early Transcendentals Functions 4TH edition},
   title = {Calculus Early Transcendentals Functions},
   volume = {53},
   year = {2012},
}
@book{Stewart2012,
   abstract = {Reading a calculus textbook is different from reading a newspaper or a novel, or even a physics book. Don’t be discouraged if you have to read a passage more than once in order to understand it. You should have pencil and paper and calculator at hand to sketch a diagram or make a calculation. Some students start by trying their homework problems and read the text only if they get stuck on an exercise. I suggest that a far better plan is to read and understand a section of the text before attempting the exercises. In particular, you should look at the definitions to see the exact meanings of the terms. And before you read each example, I suggest that you cover up the solution and try solving the problem yourself. You’ll get a lot more from looking at the solution if you do so. Part of the aim of this course is to train you to think logically. Learn to write the solutions of the exercises in a connected, step-by-step fashion with explanatory sentences—not just a string of disconnected equations or formulas. The answers to the odd-numbered exercises appear at the back of the book, in Appendix E. Some exercises ask for a verbal explanation or interpretation or description. In such cases there is no single correct way of expressing the answer, so don’t worry that you haven’t found the definitive answer. In addition, there are often several different forms in which to express a numerical or algebraic answer, so if your answer differs from mine, don’t immediately assume you’re wrong. For example, if the answer given in the back of the book is and you obtain , then you’re right and rationalizing the denominator will show that the answers are equivalent. The icon ; indicates an exercise that definitely requires the use of either a graphing calculator or a computer with graphing software. But that doesn’t mean that graphing devices can’t be used to check your work on the other exercises as well. The symbol is reserved for problems in which the full resources of a computer algebra system (like Derive, Maple, Mathematica, or the TI-89/92) are required. You will also encounter the symbol |, which warns you against committing an error. I have placed this symbol in the margin in situations where I have observed that a large proportion of my students tend to make the same mistake. The CD-ROM Tools for Enriching™ Calculus is referred to by means of the symbol . It directs you to Visuals and Modules in which you can explore aspects of calculus for which the computer is particularly useful. TEC also provides Homework Hints for representative exercises that are indicated by printing the exercise number in blue: . These homework hints ask you questions that allow you to make progress toward a solution without actually giving you the answer. You need to pursue each hint in an active manner with pencil and paper to work out the details. If a particular hint doesn’t enable you to solve the problem, you can click to reveal the next hint. (See the front endsheet for information on how to purchase this and other useful tools.) The Interactive Video Skillbuilder CD-ROM contains videos of instructors explaining two or three of the examples in every section of the text. (The symbol has been placed beside these examples in the text.) Also on the CD is a video in which I offer advice on how to succeed in your calculus course. I also want to draw your attention to the website www.stewartcalculus.com. There you will find an Algebra Review (in case your precalculus skills are weak) as well as Additional Examples, Challenging Problems, Projects, Lies My Calculator and Computer Told Me (explaining why calculators sometimes give the wrong answer), History of Mathematics, Additional Topics, chapter quizzes, and links to outside resources. I recommend that you keep this book for reference purposes after you finish the course. Because you will likely forget some of the specific details of calculus, the book will serve as a useful reminder when you need to use calculus in subsequent courses. And, because this book contains more material than can be covered in any one course, it can also serve as a valuable resource for a working scientist or engineer. Calculus is an exciting subject, justly considered to be one of the greatest achievements of the human intellect. I hope you will discover that it is not only useful but also intrinsically beautiful.},
   author = {James Stewart},
   journal = {Brooks/Cole},
   title = {Essential Calculus: Early Transcendentals},
   year = {2012},
    publisher = {online},

}
@article{Rivero2022,
   abstract = {Based on a solid mathematical background, this paper proposes a method for Symbolic Regression that enables the extraction of mathematical expressions from a dataset. Contrary to other approaches, such as Genetic Programming, the proposed method is deterministic and, consequently, does not require the creation of a population of initial solutions. Instead, a simple expression is grown until it fits the data. This method has been compared with four well-known Symbolic Regression techniques with a large number of datasets. As a result, on average, the proposed method returns better performance than the other techniques, with the advantage of returning mathematical expressions that can be easily used by different systems. Additionally, this method makes it possible to establish a threshold at the complexity of the expressions generated, i.e., the system can return mathematical expressions that are easily analyzed by the user, as opposed to other techniques that return very large expressions.},
   author = {Daniel Rivero and Enrique Fernandez-Blanco and Alejandro Pazos},
   doi = {10.1016/j.eswa.2022.116712},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   title = {DoME: A deterministic technique for equation development and Symbolic Regression},
   volume = {198},
   year = {2022},
}
@article{Makke2024,
   abstract = {Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery tool, achieving significant advances in various application domains ranging from fundamental to applied sciences. In this survey, we present a structured and comprehensive overview of symbolic regression methods, review the adoption of these methods for model discovery in various areas, and assess their effectiveness. We have also grouped state-of-the-art symbolic regression applications in a categorized manner in a living review.},
   author = {Nour Makke and Sanjay Chawla},
   doi = {10.1007/s10462-023-10622-0},
   issn = {15737462},
   issue = {1},
   journal = {Artificial Intelligence Review},
   title = {Interpretable scientific discovery with symbolic regression: a review},
   volume = {57},
   year = {2024},
}
@inproceedings{Worm2014,
   abstract = {Symbolic Regression (SR) is the data driven search for mathematical relations as performed by a computer. In essence, SR is a search over all possible equations to find those which best model the data on hand. Prioritized Grammar Enumeration (PGE) is a recently proposed algorithm which has been shown to have great efficacy and efficiency on the Symbolic Regression problem, using just a single compute core. PGE reformulates the SR problem as a search over a grammar, makes reductions in the magnitude of the search space, and introduces mechanisms for exploring that space efficiently. Notably, PGE provides reliability and reproducibility of results, a key aspect to any system used by scientists at large. In this paper, we enhance the PGE algorithm in several ways. First, we extend PGE to discover differential equations. Second, we incorporate multiple prioritization heaps into PGE, reducing point evaluations while maintaining efficacy. Finally, we decouple the PGE subroutines into a set of services, contain each with Docker, and deploy them onto the cloud. Our algorithm experiments cover a range of dynamical systems from a multitude of domains. and our cloud experiments explore a variety of architectural setups. Our results show PGE to have great promise and efficacy in automating the discovery of equations at the scales needed by tomorrow's scientific data problems.},
   author = {Tony Worm and Kenneth Chiu},
   doi = {10.1109/BigData.2014.7004284},
   booktitle = {Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014},
   title = {Scaling up Prioritized Grammar Enumeration for scientific discovery in the cloud},
   year = {2014},
}
@inproceedings{Cranmer2020,
   abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example—a detailed dark matter simulation—and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
   author = {Miles Cranmer and Alvaro Sanchez-Gonzalez and Peter Battaglia and Rui Xu and Kyle Cranmer and David Spergel and Shirley Ho},
   issn = {10495258},
   booktitle = {Advances in Neural Information Processing Systems},
   title = {Discovering symbolic models from deep learning with inductive biases},
   volume = {2020-December},
   year = {2020},
}
@article{Aldeia2022,
   abstract = {In some situations, the interpretability of the machine learning models plays a role as important as the model accuracy. Interpretability comes from the need to trust the prediction model, verify some of its properties, or even enforce them to improve fairness. Many model-agnostic explanatory methods exists to provide explanations for black-box models. In the regression task, the practitioner can use white-boxes or gray-boxes models to achieve more interpretable results, which is the case of symbolic regression. When using an explanatory method, and since interpretability lacks a rigorous definition, there is a need to evaluate and compare the quality and different explainers. This paper proposes a benchmark scheme to evaluate explanatory methods to explain regression models, mainly symbolic regression models. Experiments were performed using 100 physics equations with different interpretable and non-interpretable regression methods and popular explanation methods, evaluating the performance of the explainers performance with several explanation measures. In addition, we further analyzed four benchmarks from the GP community. The results have shown that Symbolic Regression models can be an interesting alternative to white-box and black-box models that is capable of returning accurate models with appropriate explanations. Regarding the explainers, we observed that Partial Effects and SHAP were the most robust explanation models, with Integrated Gradients being unstable only with tree-based models. This benchmark is publicly available for further experiments.},
   author = {Guilherme Seidyo Imai Aldeia and Fabrício Olivetti de França},
   doi = {10.1007/s10710-022-09435-x},
   issn = {15737632},
   issue = {3},
   journal = {Genetic Programming and Evolvable Machines},
   title = {Interpretability in symbolic regression: a benchmark of explanatory methods using the Feynman data set},
   volume = {23},
   year = {2022},
}
@article{Schmidt22009,
   abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the "alphabet" used to describe those systems.},
   author = {Michael Schmidt and Hod Lipson},
   issn = {0036-8075},
   issue = {5923},
   journal = {Science (New York, N.Y.)},
   title = {Supporting Online Material for Distilling free-form natural laws from experimental data.},
   volume = {324},
   year = {2009},
}
@article{Hope2023,
   author = {Tom Hope and Doug Downey and Daniel S. Weld and Oren Etzioni and Eric Horvitz},
   doi = {10.1145/3576896},
   issn = {15577317},
   issue = {8},
   journal = {Communications of the ACM},
   title = {A Computational Inflection for Scientific Discovery},
   volume = {66},
   year = {2023},
}
@book{kepler,
   abstract = {Originally published in English in 1973. This volume traces the development of the revolution which so drastically altered man’s view of the universe in the sixteenth and seventeenth centuries. The “astronomical revolution” was accomplished in three stages, each linked with the work of one man. With Copernicus, the sun became the centre of the universe. With Kepler, celestial dynamics replaced the kinematics of circles and spheres used by Copernicus. With Borelli the unification of celestial and terrestrial physics was completed by abandonment of the circle in favour the straight line to infinity.},
   author = {Alexandre Koyré and R. E.W. Maddison},
   doi = {10.4324/9780203706312},
   journal = {The Astronomical Revolution: Copernicus - Kepler - Borelli},
   title = {The astronomical revolution: Copernicus - Kepler - Borelli},
   volume = {20},
   year = {2013},
   publisher = {online}, 
}
@article{Schmidt2009,
   abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the -alphabet- used to describe those systems.},
   author = {Michael Schmidt and Hod Lipson},
   doi = {10.1126/science.1165893},
   issn = {00368075},
   issue = {5923},
   journal = {Science},
   title = {Distilling free-form natural laws from experimental data},
   volume = {324},
   year = {2009},
}
@article{Wood2022,
   abstract = {Researchers say we’re on the cusp of “GoPro physics,” where a camera can point at an event and an algorithm can identify the underlying physics equation},
   author = {Charlie Wood},
   journal = {Quanta Magazine},
   title = {Powerful ‘ Machine Scientists ’ Distill the Laws of Physics From Raw Data},
   year = {2022},
}
